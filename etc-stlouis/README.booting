External Links
--------------
Lab machine guide:
https://github.com/oxidecomputer/meta/blob/master/engineering/lab/gimlet.adoc

Setting up and using a helios-dev build machine:
https://github.com/oxidecomputer/stlouis/blob/main/dev.adoc

Boot Methods
------------

There are currently three ways being regularly used to boot these bits on a
Gimlet:

- bldb loader + UFS ramdisk + optional phase2 via the network;
- bldb loader + cpio miniroot + ZFS root loaded via the network or from an
  M.2 device;
- Stage 1 (phbl loader + cpio miniroot) flashed into SPI + ZFS root loaded from
  an M.2 device (the intended boot path - see RFD241).

This isn't an exhaustive list, other combinations are possible with a bit of
juggling.

The bulk of this file describes how to get going with (1), but (2) is
becoming increasingly useful for development as it produces a booted machine
that is effectively the same as (3) but elides the need to keep rewriting
the SPI flash and is easier to iterate on.

Startup Options
---------------

The Service Processor maintains a set of boot options that the Host reads
during boot. These options are stored as a single 64-bit value where the bits
have the following meanings:

  Bit	Mnemonic		Value	Note
  ---	--------		-----	-----
  0	STARTUP_RECOVERY	0x1	Request the phase 2 image via the SP
  1	STARTUP_KBM		0x2	Set kbm_debug
  2	STARTUP_BOOTRD		0x4	Set bootrd_debug
  3*	STARTUP_PROM		0x8	Set prom_debug
  4*	STARTUP_KMDB		0x10	Load KMDB
  5	STARTUP_KMDB_BOOT	0x20	Load KMDB and enter at boot
  6	STARTUP_BOOT_RAMDISK	0x40	Use the provided ramdisk directly
  7	STARTUP_BOOT_NET	0x80	Retrieve the phase 2 image via network
  8*	STARTUP_VERBOSE		0x100	Enable verbose boot

The options which have a * after their bit number are currently enabled by
default **in a lab SP image** when the SP's control-plane-agent task starts.
The value can be queried and adjusted via hiffy:

    % pfexec humility -t sn6 hiffy -c ControlPlaneAgent.get_startup_options
    ControlPlaneAgent.get_startup_options() => HostStartupOptions { bits: 0x18 }

To enable entering KMDB at boot, for example:

    % pfexec humility -t sn6 hiffy -c ControlPlaneAgent.set_startup_options \
        -a startup_options=0x38
    ControlPlaneAgent.set_startup_options() => ()

The boot-time spew options (KBM, BOOTRD, and PROM) are for bringup only and
should never be enabled in production.  kbm_debug in particular is extremely
verbose and intended only for debugging the memory mapping and allocation code
used during boot; booting will take hours with this option enabled.  Setting
STARTUP_KBM this way may not be sufficient for your debugging needs, even if
IPCC is available; see the additional notes in sys/boot_debug.h.  All three of
these options are useful mainly on new or experimental hardware or when making
substantial changes to machdep boot code; if the target machine can reach
userland there is no reason to enable them.  STARTUP_VERBOSE corresponds to
the common illumos `-v` kernel option and provides a moderate amount of
generally useful debugging spew, and this option never requires code changes
to work.  If you are experiencing a boot hang during engineering activity,
this is the place to start.  Setting this option in production is possible
with SP software cooperation but is strongly discouraged; boot hangs should be
debugged completely and eliminated prior to shipping, and once in userland,
support bundles are more appropriate to investigating other problems.

bldb loader + UFS ramdisk + optional phase2 via the network
-----------------------------------------------------------

You will need:

- A supportable Gimlet, Ruby, or an Ethanol-X, with Milan, Genoa, or
  Turin processor(s)
- The `stlouis` branch of illumos from
  [oxidecomputer/illumos-gate](https://github.com/oxidecomputer/illumos-gate)
- [bldb](https://github.com/oxidecomputer/bldb)
- [pinprick](https://github.com/oxidecomputer/pinprick) to compress your
  ramdisk
- A built stlouis workspace
- The tools in this directory

Steps:

1. Build the kernel, modules, and kmdb and its dependencies.  If you aren't
sure how to do this, don't know what adjuncts are, or just want the simplest
possible HOWTO, you are or probably want to be on helios-dev and should follow
https://github.com/oxidecomputer/stlouis/blob/main/dev.adoc.  Otherwise it is
assumed that you have a working build environment and know how to build what's
needed on your distribution using the in-gate tools.  If you want to build on
an arbitrary build machine using adjuncts, consider applying
illumos-build-fixes.diff from this directory which fixes several upstream bugs
that interfere with reproducible builds.  You don't need this patch if you are
not doing a reproducible build (again, if you aren't sure, just follow
https://github.com/oxidecomputer/stlouis/blob/main/dev.adoc).

2. Build a compressed ramdisk.  This program outputs the name of the ramdisk
image; it uses a temporary name because even though these are internal build
machines and basically single-user we try to avoid glaring security holes.  If
you want to use a fixed name you can do something like:

	$ pinprick $(./mkrd.bash) > /path/to/ramdisk.ufs.z

3. The collection of available phase-1 system software in the ramdisk is very
limited and heavily skewed toward debugging tools.  If you want a more
functional (RFD 241 "phase-2") system, you will need to unpack additional
software.  You can build a phase-2 software bundle by doing:

	$ xz $(./mkphase2.bash) > /path/to/phase2.tar.xz

This step is optional and requires either a NIC or some other way to get these
bits onto the machine after booting into phase-1.

4. Start your terminal emulator and attach it to the target machine's UART0.
The examples here assume you use picocom, but you can use whatever you like.

	// For lab systems:
	$ pfexec humility -t <target> exec console
	// In general:
	$ picocom -s 'sz -w1024 -b' -f h -b 3000000 /dev/my/local/uart

5. If you haven't done so already, build an image with `bldb` as the payload
and burn it to your test machine's boot flash.  See the instructions in the
[bldb README.md](https://github.com/oxidecomputer/bldb/blob/main/README.md)
for instructions.

6. Put the test machine into A0:

	// Image burn if needed; lab systems need pfexec + -t <target>
	$ humility qspi -D ~/src/amd-host-image-builder/milan-gimlet-b.img
	// Power up; lab systems need pfexec + -t <target>
	$ humility hiffy -c Sequencer.set_state -a state=A0

If you are using Ethanol-X or a Ruby, see the manual for your workstation's IPMI
client tools and the Ethanol-X/Ruby BMC firmware.

7. At this point, the machine should come up (it can take a few minutes!)
and you should have:
	- A box sitting at the `@` prompt in your terminal emulator
	- A copy of ramdisk.ufs.z or whatever you called it
	- Optionally, a copy of phase2.tar.xz or whatever you called it

8. If you are using a rev A Gimlet and want a NIC and/or SSD(s), you must now
power on any sharkfin power controllers you need, then release PERST# for each
PCIe end device you wish to use.  Releasing PERST# requires hardware
modification; ask if you don't know what to do.  These steps may be done in
parallel with ramdisk loading (next step).  This is not needed on rev B and
later.

9. Configure the Host startup options to boot using the ramdisk directly, and
so that it does not look for a phase 2 image. See "Startup Options" above.
For example:

    % pfexec humility -t sn6 hiffy -c ControlPlaneAgent.set_startup_options \
        -a startup_options=0x58

10. Now issue the `zoxboot` command to `bldb` and send the compressed ramdisk
via ZMODEM.  If the transfer is successful, the machine should automatically
boot the host operating system.

Note that `zoxboot` is an alias that expands to the following sequence of loader
commands, instructing `bldb` to receive the ramdisk via ZMODEM, decompress it,
mount it, load the kernel from it, and call the kernel's entry point with the
ramdisk address and size as parameters:

    call . load /platform/oxide/kernel/amd64/unix . mount . @inflate . rz

`bldb` will begin the ZMODEM handshake.  You should now use your terminal
emulator's ZMODEM send command to transfer the compressed ramdisk you built
in step 2.  On picocom, this is C-a C-s.

The `rz` command causes `bldb` to receive the compressed ramdisk and copy
it into a reserved region of memory owned by the loader.  The `@inflate`
command uncompresses the received ramdisk and writes the inflated contents
into a similarly reserved internal region; the `@` prefix causes the loader
to store an extra copy of the inflated data's location and size.  The `mount`
command makes the UFS filesystem in the inflated ramdisk accessible to `bldb`.
The `load` command reads the named ELF file from the ramdisk and loads and
maps its loadable segments into memory, returning the ELF entry point of the
loaded object.  The `call` command invokes that entry point via an x86 `CALL`
instruction, passing the address and location of the ramdisk saved earlier.

If the expanded ramdisk is larger than 128 MiB, you will also need to map
memory manually and inflate into that.  See the online help for the `map`
and `inflate` commands.

If all goes well, you should wind up with `unix` running.  Note, if you
prefer the "pipe" syntax, you can type the above as:

    rz | @inflate | mount | load /platform/oxide/kernel/amd64/unix | call

Note that due to a hardware erratum involving physical address aliasing on
Turin, there is a 64KiB hole at 0x1_0000_0000 that always reads as all-bits
1.  Beware trying to receive or inflate anything into that range.  The default
regions built into `bldb` for receive and the inflated ramdisk avoid this area
and are known to work on all of our machines.

Also, if ZMODEM is not sufficiently reliable, `bldb` supports XMODEM.  See the
[README.md](https://github.com/oxidecomputer/bldb/blob/main/README.md) in the
`bldb` repository for details on how to use it.

11. Upon reaching userland, if you have a NIC you can plumb up your network by
uttering '/root/start-network <IPv4 address>/<IPv4 CIDR>'.  This will also
start sshd for remote access.  If you require access from an off-link network,
add routes as appropriate at this time.

12. If you want the additional phase-2 software you build in step 3, copy it
onto the test machine now and run the included script to get it mounted:

    $ xzcat phase2.tar.xz | ssh -C root@gimlet 'tar xpf - -C /tmp'
	gimlet:~# /root/post-phase2

The root user's environment is set up so that the stuff in /tmp/bin and so
forth will be on the PATH and libraries can generally be found.  You can of
course add or change the contents to suit your needs; the above will be enough
to give you working zpool and zfs commands so if you have storage you can now
create or import a pool, making it convenient to persist data across reboots
for debugging or V&V, store larger toolsets, etc.

One thing you can do from here is partition the M.2 SSD in slot 17 and copy
the contents of an IPS-style phase-2 image ("zfs.img") into the first
partition.  This will allow you to boot using either of the alternate methods
described in subsequent sections without needing an Ethernet boot server.
Note that the phase-2 image you copy onto the SSD needs to match the cpio
archive you boot; see subsequent sections for more details if you wish to try
either of these approaches to booting.

13. If you wish to enter kmdb, send a BREAK using your terminal emulator
(picocom default: C-a C-|).  NOTE: Sending BREAK via the USB FTDI driver on
the illumos lab machines does not work and will do nothing.  The 'reboot'
command should do what you'd expect; if not, use humility to cycle through A2
on Gimlet.  On Eth-X, ipmitool or the web UI may be used to reboot.

bldb loader + cpio miniroot + ZFS root loaded via the network
-------------------------------------------------------------

You will need most of the same things as for the previous method, including a
system which has been flashed with `bldb` and a machine in A0 sitting at
bldb's `@` prompt in your terminal emulator.

You will also need a cpio miniroot archive and ZFS root image built with
the helios-build tool - https://github.com/oxidecomputer/helios - see below.

The following steps show how to build DEBUG pieces, as should generally be used
during development:

Steps:

1. Check out the helios repository and follow the Getting Started guide
in that repository's README.md to build the tools and set up additional
workspaces.

2. Build a stlouis workspace using helios-build.

	./helios-build build-illumos -dq

3. Build the installation image files:

	./helios-build image -d

This will produce a set of output files under image/output:

	cpio	- cpio miniroot archive (not needed for this boot method)
	cpio.z	- pinprick-compressed version of above
	rom	- SPI flash image (not needed for this boot method)
	unix.z	- pinprick-compressed kernel (not needed for this boot method)
	zfs.img	- ZFS root image

4. Start up a network bootserver on a lab machine which will serve the ZFS
root image.

	./projects/bootserver/target/release/bootserver \
	    e1000g0 image/output/zfs.img 18:c0:4d:11:22:33

In this example, e1000g0 is the name of the NIC on the lab machine and
18:c0:4d:11:22:33 is the MAC address of the NIC in the Gimlet which will
be used for booting. The NIC in the Gimlet is usually connected directly
via a sharkfin slot or via a U.2 slot in the front of the machine.

5. Configure the Host startup options to boot using the network.
See "Startup Options" above.

    % pfexec humility -t sn6 hiffy -c ControlPlaneAgent.set_startup_options \
        -a startup_options=0x98

6. Boot

Use `zoxboot` as before and send the compressed cpio archive via ZMODEM.

Or instruct `bldb` to receive and decompress the initial cpio miniroot image,
locate the copy of `unix` in it, load that, and call into the kernel by issuing
the following commands explicitly.  Note that this is the exact same sequence of
commands as with the UFS root:

	call . load /platform/oxide/kernel/amd64/unix . mount . @inflate . rz

`bldb` will begin the ZMODEM transfer and wait for you to send the
compressed cpio miniroot.  You should now use your terminal emulator's
ZMODEM send command to transfer that image, image/output/cpio.z, created
in an earlier step.  With picocom, this is C-a C-s.

Again, if you prefer the pipe syntax, this is the same as:

	rz | @inflate | mount | load /platform/oxide/kernel/amd64/unix | call

When this transfer completes, unix should start automatically, in a manner
similar to the method with a UFS ramdisk, mentioned above.

Once the kernel boots, you will see it try to find the ZFS image on an
M.2 drive, and then falling back to network boot and loading the image from
the bootserver that was started earlier. Here are some of the messages that
you should expect to see:

	in oxide_boot!
	    cpio wants: c87d... (ZFS image hash)
	TRYING: boot disk
	found M.2 device @ /pci@0,0/pci1022,1483@1,3/pci1...
	opening M.2 device
	invalid disk header
	closing M.2
	TRYING: boot net
	MAC address is 18:c0:4d:11:22:33
	listening for packets...
	hello...
	    in image: c87d...
	received offer from 34:17:eb:d4:93:a4  -- size 4294967296 data size 367001600
	reached EOF at offset 367001600 after 4 seconds
	all done!
	checksum ok!

At this point the system should complete booting and provide a login prompt.

Stage 1 (phbl loader + cpio miniroot) flashed into SPI + ZFS root...
--------------------------------------------------------------------

This boot method is very similar to the previous one except that the
image/output/rom file is written to the SPI flash in place of the image
containing `bldb`. This image includes the `phbl` boot loader and the
cpio miniroot image.

	$ pfexec humility -t sn6 qspi -D image/output/rom

No interaction is necessary for the system to boot, it will automatically
start up and look for a phase 2 image on the M.2 disk that corresponds to the
boot storage unit in use. If you wish to load from the network then the
corresponding bit needs to be set in the SP's startup options - See "Startup
Options" above. Note that the hash of the ZFS image must match the value
embedded in the cpio archive, which is part of the ROM, and so this method is
not particularly suited for iterative work on stlouis.

M.2 setup
---------

When developing on a Gimlet it is convenient to configure an M.2 device to hold
a persistent ZFS pool, the system dump device, and some partitions for holding
a phase2 image.

The layout which is currently recommended is GPT with the following partitions:

    Part      Tag    Flag     First Sector         Size         Last Sector
      0        usr    wm                 6        4.00GB          1048581
      1        usr    wm           1048582        4.00GB          2097157
      2        usr    wm           2097158        4.00GB          3145733
      3        usr    wm           3145734        4.00GB          4194309
      4        usr    wm           4194310      500.00GB          135266309
      5        usr    wm         135266310      500.00GB          266338309
      6 unassigned    wm                 0           0               0
      8   reserved    wm         468841553        8.00MB          468843600

Many of the machines in the lab have a ZFS pool called 'data' on partition 5
and use partition 4 for their dump device. A phase2 image can be written to
partition 0 if required and the oxide_boot module will look for it there.
With a 'data' pool in place, the site/postboot service in the ZFS root image
will automatically import it and then run '/data/postboot/bin/postboot.sh'
if it exists. This is useful in order to run tasks on every boot such as
setting up the system dump device.

* https://github.com/oxidecomputer/helios/blob/master/image/templates/files/gimlet-postboot.sh

Anonymous DTrace support
------------------------

Anonymous DTrace is supported on a gimlet and should work transparently, in the
same way as on a traditional disk root system, via the dtrace(8) command with
the `-A` option.

More information on Anonymous DTrace can be found in the DTrace book at
     https://illumos.org/books/dtrace/chp-anon.html#chp-anon

On a gimlet, this stores a compressed version of `/etc/system` and
`/kernel/drv/dtrace.conf` in dedicated slots in the SP's key value store.

    gimlet# dtrace -An 'fbt::bd_xfer_done:entry/arg1/{stack(); print(*args[0])}'
    dtrace: saved anonymous enabling in /kernel/drv/dtrace.conf
    dtrace: added forceload directives to /etc/system
    dtrace: run update_drv(8) or reboot to enable changes
    Successfully stored '/etc/system' in SP
    Successfully stored '/kernel/drv/dtrace.conf' in SP

On the next boot, these saved files are retrieved and used in place of any
corresponding files in the ramdisk image. Depending on verbosity settings,
you will see these files being accessed during boot, and the associated
IPCC transactions.

    ipcc: opened '/etc/system', 0x6b bytes
    ipcc: decompressed to 0x178 bytes
    ipcc: opened '/kernel/drv/dtrace.conf', 0x2f7 bytes
    ipcc: decompressed to 0x2553 bytes

It is possible to view the stored values using the `ipcc` utility:

    gimlet# /usr/platform/oxide/bin/ipcc keylookup system
    (length 78)
    0000   af 00 78 da 4b cb 2f 4a 4e cd c9 4f 4c b1 52 48  |..x.K./JN..OL.RH|
    0010   29 2a d3 2f ae 2c 2e 29 4a 4c 4e e5 4a 43 13 4f  |)*./.,.)JLN.JC.O|
    0020   29 41 17 2a 28 ca 4f cb cc c1 50 99 93 9f 9c 5d  |)A.*(.O...P....]|
    0030   5c 92 88 a1 3c 2d 09 53 28 b1 b8 04 68 5b 01 ba  |\...<-.S(...h[..|
    0040   78 4a 72 41 32 86 18 c4 59 00 47 82 40 2e        |xJrA2...Y.G.@.  |

or, in the decompressed form:

    gimlet# /usr/platform/oxide/bin/ipcc keylookup -c system
    (length 175)
    0000   66 6f 72 63 65 6c 6f 61 64 3a 20 64 72 76 2f 73  |forceload: drv/s|
    0010   79 73 74 72 61 63 65 0a 66 6f 72 63 65 6c 6f 61  |ystrace.forceloa|
    0020   64 3a 20 64 72 76 2f 73 64 74 0a 66 6f 72 63 65  |d: drv/sdt.force|
    0030   6c 6f 61 64 3a 20 64 72 76 2f 70 72 6f 66 69 6c  |load: drv/profil|
    0040   65 0a 66 6f 72 63 65 6c 6f 61 64 3a 20 64 72 76  |e.forceload: drv|
    0050   2f 6c 6f 63 6b 73 74 61 74 0a 66 6f 72 63 65 6c  |/lockstat.forcel|
    0060   6f 61 64 3a 20 64 72 76 2f 66 62 74 0a 66 6f 72  |oad: drv/fbt.for|
    0070   63 65 6c 6f 61 64 3a 20 64 72 76 2f 66 61 73 74  |ceload: drv/fast|
    0080   74 72 61 70 0a 66 6f 72 63 65 6c 6f 61 64 3a 20  |trap.forceload: |
    0090   64 72 76 2f 64 63 70 63 0a 66 6f 72 63 65 6c 6f  |drv/dcpc.forcelo|
    00a0   61 64 3a 20 64 72 76 2f 64 74 72 61 63 65 0a     |ad: drv/dtrace. |

Undoing this, and disabling anonymous DTrace is just the same as on a
traditional system, via the dtrace(8) command with only the `-A` option.

    gimlet# dtrace -A
    dtrace: cleaned up old anonymous enabling in /kernel/drv/dtrace.conf
    dtrace: cleaned up forceload directives in /etc/system
    Clearing '/etc/system' from SP
    Clearing '/kernel/drv/dtrace.conf' from SP

If the system is unable to boot due to these settings, they can be cleared by
restarting the `host_sp_comms` task in the SP.

    butler% pfexec humility jefe -f host_sp_comms
    butler% pfexec humility jefe -s host_sp_comms

after which they will be empty:

    gimlet# /usr/platform/oxide/bin/ipcc keylookup system
    (length 0)

An SP reset will also achieve this, but will put the host into A2 - not
really a problem if it is in a boot loop - and leave it there if the SP is
running a lab image.

In a rack environment, without debug probes, one would use one of:

 -  pilot sp exec -e reset <cubby|ip|serial>
 -  humility -a <archive> -i <SP IP> reset

Persistent /etc/system support
------------------------------

As a benefit of the anonymous DTrace support, there is also a limited ability
to set things persistently in `/etc/system`. There are only 256 bytes available
in the SP to store the compressed file so generally you will have to strip out
all of the comments in order for it to fit.

Here is an example of setting such a variable persistently to support testing
and avoiding the need to manually set things from kmdb on each boot:

    gimlet# ls -l /etc/system
    -rw-r--r--   1 root     sys         3814 Dec 28 02:18 /etc/system
    gimlet# sed -i '/^\*/d; /^$/d' /etc/system
    gimlet# ls -l /etc/system
    -rw-r--r--   1 root     sys          175 Dec 28 02:32 /etc/system
    gimlet# echo set ip:ip_squeue_fanout=1 >> /etc/system
    gimlet# /usr/platform/oxide/bin/ipcc keyset -c system /etc/system
    Success

To undo this, you can set the file to `/dev/null`:

    gimlet# /usr/platform/oxide/bin/ipcc keyset system /dev/null

If changes to this file render the system unbootable, it can be cleared by
restarting the SP's `host_sp_comms` task or resetting the SP as described in
the section on Anonymous DTrace above.

Using nanobl-rs
---------------

The `nanobl-rs` loader is another loader that you may encounter on some
machines.  Using it is substantially similar to `bldb`, but the command syntax
is slightly different and it doesn't support loading kernels from ramdisks; to
load and boot a host OS, you must therefore transfer the ramdisk as well as the
matching kernel image.

Also, `nanobl-rs` only supports XMODEM, not ZMODEM, and does not support the
`bldb` trick of reading a single byte before starting the transfer on the
receive side: instead, it uses a timeout.  You must initiate the transfer within
a few seconds or you will miss the opportunity and will have to power cycle the
machine you want to boot: there is no way to abort receiving once you've
started.

You can boot with the same configurations of ramdisk types (UFS, cpio miniroot,
and so on) as with `bldb`.  Also as with `bldb`, if you just want to see if the
kernel runs at all, even if it panics very early on, you can eschew transferring
a ramdisk entirely.

The steps to build images and prepare artifacts for boot are the same as with
`bldb`, save that you must also have a copy of the compressed kernel, which is
contained in the `unix.z` file mentioned above, and is separate from the
ramdisk.  Like `bldb`, `nanobl-rs` supports compression, which greatly reduces
the amount of time to send artifacts over the UART, though it is optional.

To boot a machine with `nanobl-rs`, you will use two separate commands.  First,
to transfer the compressed ramdisk, issue the following:

    > 110000000::recv -m | ::inflate 101000000

nanobl-rs will block, silently waiting for data to arrive.  You should now use
your terminal emulator's XMODEM send command to transfer the compressed ramdisk
you built in step 2.  With picocom, the key sequence to initiate transfer is C-a
C-s, but you must use `sx`, not `sz`.

This command places the ramdisk into memory at 1_0100_0000.  Note that if you
build a ramdisk larger than 128 MiB, you will also need to map memory manually
or specify the size in bytes on the loader command line.  See the help for
::recv's -m option and ::inflate.

A suitable invocation of picocom that configures it for use with XMODEM and the
correct options is:

    picocom -s 'sx -Xk' -f h -b 3000000

If you did not compress the ramdisk, then the following suffices to receive the
ramdisk:

    > 10100000::recv -m

Transfers of an uncompressed ramdisk take significantly longer, however, so this
is rare.

When the ramdisk transfer completes, the address and size of the uncompressed
image will be displayed, separated with a comma.  These values should be used as
the parameters of the ::call command in the next step.  Unlike with `bldb`, you
must manually pass these parameters to `::call`.

    *** exit status: 0 ***
    101000000,4c00000

Next, instruct nanobl-rs to receive `unix.z` in the same manner, and to begin
executing the kernel once the transfer completes.  If you transfer the
compressed kernel, utter:

    > 110000000::recv -m | ::inflate 111000000 | ::load | ::call 101000000 4c00000

If you did not compress your kernel, you can simply omit the `::inflate`
command; the default receive address in this case is fine.

Again, as soon as nanobl-rs blocks, use XMODEM to send unix.z (or unix) to the
target machine.

The addresses used for `unix.z` and the ramdisk in the above examples are almost
entirely arbitrary, but should work on any machine we have.  When the file
receiption completes, `::load` and `::call` will interpret the kernel as an ELF
object and call its entry point, booting the OS, passing it the address and size
of the ramdisk as arguments.
